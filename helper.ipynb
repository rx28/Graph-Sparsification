{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DU-2qUEu_r0c"
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random\n",
    "import community as comm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mIqNCDof_5lP"
   },
   "outputs": [],
   "source": [
    "# Function to create a GNN module to run a 2-layer network for GCN, GraphSage and GAT\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_feats, val, dropout_rate, classes):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if val == 0:\n",
    "            self.conv1 = GCNConv(in_feats, 16)\n",
    "            self.conv2 = GCNConv(16, classes)\n",
    "        elif val == 1:\n",
    "            self.conv1 = SAGEConv(in_feats, 16)\n",
    "            self.conv2 = SAGEConv(16, classes)\n",
    "        else:\n",
    "            self.conv1 = GATConv(in_feats, 16)\n",
    "            self.conv2 = GATConv(16, classes)\n",
    "            \n",
    "        self.dropout_rate = dropout_rate\n",
    " \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the node embeddings by running desired GNN model\n",
    "\n",
    "def gnn_embed(dataset, gnn_val=0):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Net(in_feats = dataset.num_node_features, val = gnn_val, dropout_rate=0.05, classes = dataset.num_classes).to(device)\n",
    "    data = dataset[0].to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "        acc = correct / data.train_mask.sum().item()\n",
    "        print('Epoch: %d, Train Accuracy: %.4f'%(epoch,acc))\n",
    "\n",
    "        if acc == 1:\n",
    "            break\n",
    "\n",
    "    # Freeze the last layer and get node embeddings\n",
    "    model.conv2.eval()\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = model.conv1(data.x, data.edge_index)\n",
    "        node_embeddings = F.relu(node_embeddings)\n",
    "        node_embeddings = model.conv2(node_embeddings, data.edge_index)\n",
    "        node_embeddings = node_embeddings.cpu().numpy()\n",
    "\n",
    "    # Convert node embeddings to dictionary\n",
    "    node_dict = {}\n",
    "    for i in range(len(node_embeddings)):\n",
    "        node_dict[i] = node_embeddings[i]\n",
    "\n",
    "    # Get the Test Accuracy\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    print('\\nTest Accuracy: {:.4f}'.format(acc))\n",
    "        \n",
    "    # Entire Dataset accuracy\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float (pred.eq(data.y).sum().item())\n",
    "    acc = correct / len(data.x)\n",
    "    print('\\nTotal Accuracy: {:.4f}\\n'.format(acc))\n",
    "        \n",
    "    # Return node embedding    \n",
    "    return node_dict, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the Predicted probabilities of the edges based on Link Prediction heuristic\n",
    "\n",
    "def makeLinkPrediction(G, embeddings):\n",
    "    # converting embedding to a numpy array\n",
    "#     X = [[0] for i in range(graph.number_of_nodes())]\n",
    "#     for i in range(0, graph.number_of_nodes()):\n",
    "#         X[i] = embeddings[str(i+1)]\n",
    "#     X = np.array(X)\n",
    "    X = np.array([embeddings[i] for i in embeddings.keys()])\n",
    "    \n",
    "    Xd = []\n",
    "    Yd = []\n",
    "    Xdd = []\n",
    "    Ydd = []\n",
    "    count = 0\n",
    "    # for all vertices\n",
    "    vlist = []\n",
    "#     for u in range(1, G.number_of_nodes() + 1):\n",
    "    for u in range(G.number_of_nodes()):\n",
    "#         Nu = [int(nn) for nn in list(G.neighbors(str(u)))]\n",
    "        Nu = [nn for nn in list(G.neighbors(u))]\n",
    "        count += len(Nu)\n",
    "        cn = 0\n",
    "        totalns = 0\n",
    "        # for all neighbors of u\n",
    "        for n in Nu:\n",
    "            x = []\n",
    "            if n > u and ((u,n) not in vlist and (n,u) not in vlist):\n",
    "                for d in range(len(X[0])):\n",
    "                    x.append(X[u][d] - X[n][d]) # distance between the embeddings of u and its neighbor n\n",
    "                Xd.append(x)\n",
    "                Yd.append(1) # positive sample (edge present)\n",
    "                totalns += 1\n",
    "                vlist.append((u,n) if (u,n) in list(G.edges()) else (n,u))\n",
    "        tmpnn = []\n",
    "        if len(Nu) > G.number_of_nodes() // 2:\n",
    "            totalns = (G.number_of_nodes() - len(Nu)) // 2\n",
    "            #print(\"Testing neighbors!\")\n",
    "        while cn < totalns:\n",
    "            nn = random.randint(0, G.number_of_nodes() - 1)\n",
    "            # non-neighbors of u\n",
    "            if nn not in Nu and nn not in tmpnn and ((u,nn) not in vlist and (nn,u) not in vlist):\n",
    "                cn += 1\n",
    "                x = []\n",
    "                for d in range(len(X[0])):\n",
    "                    x.append(X[u][d] - X[nn][d])\n",
    "                Xdd.append(x)\n",
    "                Ydd.append(0) # negative sample (edge absent)\n",
    "                tmpnn.append(nn)\n",
    "    Xx, Yy = np.array(Xd+Xdd), np.array(Yd+Ydd)\n",
    "    Xd, Yd = np.array(Xd), np.array(Yd)\n",
    "    Xdd, Ydd = np.array(Xdd), np.array(Ydd)\n",
    "    indices = np.array(range(len(Yy)))\n",
    "    np.random.shuffle(indices)\n",
    "    Xt = Xx[indices]\n",
    "    Yt = Yy[indices]\n",
    "    \n",
    "    tf = 0.85\n",
    "    \n",
    "    CV = int(len(Yt) * tf)\n",
    "    trainX = Xt[0:CV]\n",
    "    testX = Xt[CV:]\n",
    "    trainY = Yt[0:CV]\n",
    "    testY = Yt[CV:]\n",
    "\n",
    "    # Train the Random Forest classifier using grid search to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(n_estimators=100, min_samples_split=5), {'max_depth': [None, 10, 20, 30]}, cv=5)\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    # Test the Random Forest classifier\n",
    "    modelRF = grid_search.best_estimator_.fit(trainX, trainY)\n",
    "    predictedY = modelRF.predict(testX)\n",
    "    predictedProbY = modelRF.predict_proba(Xd)[:, 1]\n",
    "\n",
    "    # Compute the accuracy score\n",
    "    acc = accuracy_score(predictedY, testY)\n",
    "    print(\"Link Prediction Accuracy:\",acc)\n",
    "   \n",
    "    return predictedProbY, vlist, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lNGGzk7mAQWt"
   },
   "outputs": [],
   "source": [
    "# Sparsify (remove edges) the graph for given percentage using the different methods as described below:\n",
    "# val = 0 ; Random Heuristic = Random deletion of edges\n",
    "# val = 1 ; Random-Link Prediction Heuristic = Randomly choose edges to delete based on the Link Prediction probabilities \n",
    "\n",
    "def sparsify_graph(G, edges, prob, perc, val):\n",
    "    \n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    num_to_delete = int(num_edges * perc)\n",
    "    deleted = [False] * num_edges  # Initialize all edges as not deleted\n",
    "    \n",
    "    if val in [0,1]:\n",
    "        \n",
    "        if val == 0:    \n",
    "            prob = [1/num_edges]*num_edges\n",
    "        \n",
    "        if val == 1:\n",
    "            prob = [i/sum(prob) for i in prob]\n",
    "        \n",
    "        edges_to_delete = np.random.choice(range(num_edges), size=num_to_delete, replace=False, p=prob)\n",
    "    \n",
    "    if val == 2:\n",
    "        edges_to_delete = list(range(num_to_delete))\n",
    "        \n",
    "    for i in edges_to_delete:\n",
    "        edge = edges[i]\n",
    "        \n",
    "        if edge in list(G.edges()):\n",
    "            G.remove_edge(*edge)\n",
    "        else:\n",
    "            edge = (edge[1], edge[0])\n",
    "            G.remove_edge(*edge)\n",
    "            \n",
    "        deleted[i] = True\n",
    "    \n",
    "    return G, deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the node labels of the graph\n",
    "\n",
    "def read_node_label(filename, skip_head=False):\n",
    "    fin = open(filename, 'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        if skip_head:\n",
    "            fin.readline()\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "        X.append(vec[0])\n",
    "        Y.append(vec[1:])\n",
    "    fin.close()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyBPK1t2AQZG",
    "outputId": "8f4cad68-82bb-43d3-e409-2674463cf3d1"
   },
   "outputs": [],
   "source": [
    "# Fuunction to get best ranker\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return np.asarray(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Classification function\n",
    "\n",
    "def node_classification(embeddings, G, dataset):\n",
    "    #X, Y = read_node_label(label,skip_head=True)\n",
    "    X=list(G.nodes)\n",
    "    Y=dataset[0].y.tolist()\n",
    "    Y=[[x] for x in Y]\n",
    "#     ltrainfrac = [0.05, 0.1, 0.2, 0.3, .4, .5, .6, .7, .8]\n",
    "    ltrainfrac = [0.8]\n",
    "    for tf in ltrainfrac:\n",
    "        print(\"Training classifier using {:.2f}% nodes...\".format(tf * 100))\n",
    "        acc = split_train_evaluate(X, Y, embeddings, tf)\n",
    "        \n",
    "    return acc\n",
    "        \n",
    "def split_train_evaluate(X, Y, embeddings, train_precent, seed=0):\n",
    "    state = np.random.get_state()\n",
    "    training_size = int(train_precent * len(X))\n",
    "    np.random.seed(seed)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
    "    X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "    Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "    X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "    Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "    # train\n",
    "    binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "    binarizer.fit(Y)\n",
    "    X_tr = [embeddings[x] for x in X_train]\n",
    "    Y_tr = binarizer.transform(Y_train)\n",
    "    clf=TopKRanker(LogisticRegression())\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    np.random.set_state(state)\n",
    "    top_k_list = [len(l) for l in Y_test]\n",
    "    X_test = np.asarray([embeddings[x] for x in X_test])\n",
    "    Y_pred = clf.predict(X_test, top_k_list=top_k_list)\n",
    "    results = {}\n",
    "    results['acc'] = accuracy_score(binarizer.transform(Y_test),Y_pred)\n",
    "    print('-------------------')\n",
    "    print(results)\n",
    "    print('-------------------')\n",
    "    \n",
    "    return results['acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Evaluation function\n",
    "\n",
    "def cluster_eval(G, embeddings):\n",
    "    # converting embedding to a numpy array\n",
    "    X = [[0] for i in range(G.number_of_nodes())]\n",
    "    for i in range(0, G.number_of_nodes()):\n",
    "        X[i] = embeddings[str(i+1)]\n",
    "    X = np.array(X)\n",
    "\n",
    "    bestModularity = 0\n",
    "    bestC = 2\n",
    "    NOC = 30\n",
    "    allmodularity = []\n",
    "    \n",
    "    for cls in range(2, NOC):\n",
    "        \n",
    "        # find clusters using a kmeans clustering algorithm on the embedding\n",
    "        # Number of clusters is set to cls\n",
    "        clusters = KMeans(n_clusters=cls, random_state=0).fit(X)\n",
    "        predG = dict()\n",
    "        for node in range(len(clusters.labels_)):\n",
    "            predG[str(node+1)] = clusters.labels_[node]\n",
    "#         return predG\n",
    "        # compute the modularity score of the Kmeans clustering\n",
    "        modularity = comm.community_louvain.modularity(predG, G)\n",
    "        allmodularity.append(modularity)\n",
    "#         print(\"Number of clusters: \", cls, \"  Modularity: \", modularity)\n",
    "        if modularity > bestModularity:\n",
    "            bestModularity = modularity\n",
    "            bestC = cls\n",
    "            \n",
    "    print(\"Best Modularity:\",bestModularity, \"Clusters:\", bestC)\n",
    "    \n",
    "    return bestModularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings of the graph \n",
    "\n",
    "def get_embedding(G, walks, embed_size=128, window_size=5, workers=3, iter=5, **kwargs):\n",
    "    kwargs[\"sentences\"] = walks\n",
    "    kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "    kwargs[\"vector_size\"] = embed_size\n",
    "    kwargs[\"sg\"] = 1  # skip gram\n",
    "    kwargs[\"hs\"] = 1  # deepwalk use Hierarchical Softmax\n",
    "    kwargs[\"workers\"] = workers\n",
    "    kwargs[\"window\"] = window_size\n",
    "    kwargs[\"epochs\"] = iter\n",
    "\n",
    "    print(\"Learning embedding vectors...\")\n",
    "    model = Word2Vec(**kwargs)\n",
    "    print(\"Learning embedding vectors done!\")\n",
    "\n",
    "    embeddings = {}\n",
    "    for word in G.nodes():\n",
    "        embeddings[word] = model.wv[word]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement DeepWalk\n",
    "\n",
    "def deepwalk_walks(G, num_walks, walk_length,):\n",
    "        nodes = G.nodes()\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            for v in nodes:\n",
    "                walk = [v]\n",
    "                while len(walk) < walk_length:\n",
    "                    cur = walk[-1]\n",
    "                    cur_nbrs = list(G.neighbors(cur))\n",
    "                    if len(cur_nbrs) > 0:\n",
    "                        walk.append(random.choice(cur_nbrs))\n",
    "                    else:\n",
    "                        break\n",
    "                walks.append(walk)\n",
    "        return walks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement Node2Vec Walk\n",
    "\n",
    "def node2vec_walks(G, p, q, num_walks, walk_length,):\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight', 1.0)\n",
    "                                  for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [\n",
    "                float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "\n",
    "        for edge in G.edges():\n",
    "            alias_edges[edge] = get_alias_edge(G, p, q, edge[0], edge[1])\n",
    "            alias_edges[(edge[1], edge[0])] = get_alias_edge(G, p, q, edge[1], edge[0])\n",
    "        \n",
    "        # generate walks\n",
    "        nodes = G.nodes()\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            for v in nodes:\n",
    "                walk = [v]\n",
    "                while len(walk) < walk_length:\n",
    "                    cur = walk[-1]\n",
    "                    cur_nbrs = list(G.neighbors(cur))\n",
    "                    if len(cur_nbrs) > 0:\n",
    "                        if len(walk) == 1:\n",
    "#                             print(\"1.1\",alias_nodes[cur][0], alias_nodes[cur][1])\n",
    "#                             print(\"1.2\",alias_sample(alias_nodes[cur][0], alias_nodes[cur][1]))\n",
    "                            walk.append(\n",
    "                                cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                        else:\n",
    "                            prev = walk[-2]\n",
    "                            edge = (prev, cur)\n",
    "#                             print(\"!\", edge)\n",
    "#                             print(\"2.1\",alias_edges[edge][0],\n",
    "#                                                       alias_edges[edge][1])\n",
    "#                             print(\"2.2\",alias_sample(alias_edges[edge][0],\n",
    "#                                                       alias_edges[edge][1]))\n",
    "                            next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
    "                                                      alias_edges[edge][1])]\n",
    "                            walk.append(next_node)\n",
    "                    else:\n",
    "                        break\n",
    "                walks.append(walk)\n",
    "        return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for Node2Vec\n",
    "\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "            (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random()*N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n",
    "    \n",
    "def get_alias_edge(G, p, q, t, v):\n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight', 1.0)  # w_vx\n",
    "            if x == t:  # d_tx == 0\n",
    "                unnormalized_probs.append(weight/p)\n",
    "            elif G.has_edge(x, t):  # d_tx == 1\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:  # d_tx > 1\n",
    "                unnormalized_probs.append(weight/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [\n",
    "            float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return create_alias_table(normalized_probs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
